{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eeb8fbda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import jieba\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# 去停用词\n",
    "def drop_stopwords(context, stop_words_list):\n",
    "    return [word for word in context if word not in stop_words_list and word != ' ']\n",
    "\n",
    "\n",
    "# 返回停用词\n",
    "def text_stop(file_path):\n",
    "    with open(file_path + 'cn_stopwords.txt', encoding='utf-8') as stop:\n",
    "        stop_words = stop.read().split(\"\\n\")\n",
    "    stop_words_list = list(stop_words)\n",
    "    stop_words_list.append(\"\\u3000\")\n",
    "    return stop_words_list\n",
    "\n",
    "\n",
    "# 文本处理\n",
    "def process_text_file(file_path, text_name, stop_words_list):\n",
    "    print(f\"Processing {text_name}\")\n",
    "    with open(file_path + \"/\" + text_name + \".txt\", \"r\", encoding='gb18030') as file:\n",
    "        all_text = file.read()\n",
    "        for ad in ['本书来自www.cr173.com免费txt小说下载站', '更多更新免费电子书请关注www.cr173.com',\n",
    "                   '她', '他', '你', '我', '它', '这', '\\u3000']:\n",
    "            all_text = all_text.replace(ad, '')\n",
    "        paragraphs = all_text.split(\"\\n\")\n",
    "        text_jieba = []\n",
    "        for para in paragraphs:\n",
    "            if para.strip() == '':\n",
    "                continue\n",
    "            processed_para = drop_stopwords(jieba.lcut(para), stop_words_list)\n",
    "            if processed_para:\n",
    "                text_jieba.append(processed_para)\n",
    "        return text_jieba\n",
    "\n",
    "\n",
    "def train_model(text_name, text_data):\n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text_data)\n",
    "    sequences = tokenizer.texts_to_sequences(text_data)\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index) + 1\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "    # Pad sequences\n",
    "    data = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    # Define the context window size\n",
    "    window_size = 2\n",
    "\n",
    "    # Prepare input and output pairs for training\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for sequence in sequences:\n",
    "        for i in range(window_size, len(sequence) - window_size):\n",
    "            context_words = sequence[i - window_size:i] + sequence[i + 1:i + window_size + 1]\n",
    "            target_word = sequence[i]\n",
    "            inputs.append(context_words)\n",
    "            labels.append(target_word)\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    input_length = 2 * window_size\n",
    "    model = create_lstm_model(input_length=input_length,vocab_size=vocab_size, embedding_dim=80)\n",
    "    # Train model\n",
    "    model.fit(inputs, labels, epochs=15, batch_size=512)\n",
    "\n",
    "    # Extract word embeddings\n",
    "    embeddings = model.layers[1].get_weights()[0]\n",
    "\n",
    "    return tokenizer, embeddings, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7f5b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_length, vocab_size, embedding_dim):\n",
    "    # 定义输入层\n",
    "    input_layer = Input(shape=(input_length,))\n",
    "    # 定义嵌入层\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length)(input_layer)\n",
    "    # 定义LSTM层\n",
    "    lstm_layer = LSTM(128)(embedding_layer)\n",
    "    # 定义输出层\n",
    "    output_layer = Dense(vocab_size, activation='softmax')(lstm_layer)\n",
    "\n",
    "    # 创建模型\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    # 编译模型\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58b2a292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 倚天屠龙记\n",
      "Epoch 1/15\n",
      "587/587 [==============================] - 58s 98ms/step - loss: 9.3211\n",
      "Epoch 2/15\n",
      "587/587 [==============================] - 59s 100ms/step - loss: 8.9326\n",
      "Epoch 3/15\n",
      "587/587 [==============================] - 60s 102ms/step - loss: 8.7471\n",
      "Epoch 4/15\n",
      "587/587 [==============================] - 60s 103ms/step - loss: 8.4412\n",
      "Epoch 5/15\n",
      "587/587 [==============================] - 62s 106ms/step - loss: 8.0589\n",
      "Epoch 6/15\n",
      "587/587 [==============================] - 67s 113ms/step - loss: 7.6619\n",
      "Epoch 7/15\n",
      "587/587 [==============================] - 68s 116ms/step - loss: 7.2541\n",
      "Epoch 8/15\n",
      "587/587 [==============================] - 68s 117ms/step - loss: 6.8504\n",
      "Epoch 9/15\n",
      "587/587 [==============================] - 69s 117ms/step - loss: 6.4595\n",
      "Epoch 10/15\n",
      "587/587 [==============================] - 70s 118ms/step - loss: 6.0847\n",
      "Epoch 11/15\n",
      "587/587 [==============================] - 71s 120ms/step - loss: 5.7225\n",
      "Epoch 12/15\n",
      "587/587 [==============================] - 71s 120ms/step - loss: 5.3723\n",
      "Epoch 13/15\n",
      "587/587 [==============================] - 71s 121ms/step - loss: 5.0288\n",
      "Epoch 14/15\n",
      "587/587 [==============================] - 71s 121ms/step - loss: 4.6987\n",
      "Epoch 15/15\n",
      "587/587 [==============================] - 71s 122ms/step - loss: 4.3790\n",
      "Processing 鹿鼎记\n",
      "Epoch 1/15\n",
      "691/691 [==============================] - 88s 127ms/step - loss: 9.1145\n",
      "Epoch 2/15\n",
      "691/691 [==============================] - 91s 131ms/step - loss: 8.6977\n",
      "Epoch 3/15\n",
      "691/691 [==============================] - 92s 133ms/step - loss: 8.4035\n",
      "Epoch 4/15\n",
      "691/691 [==============================] - 93s 134ms/step - loss: 8.0327\n",
      "Epoch 5/15\n",
      "691/691 [==============================] - 92s 134ms/step - loss: 7.6515\n",
      "Epoch 6/15\n",
      "691/691 [==============================] - 94s 136ms/step - loss: 7.2443\n",
      "Epoch 7/15\n",
      "691/691 [==============================] - 92s 133ms/step - loss: 6.8269\n",
      "Epoch 8/15\n",
      "691/691 [==============================] - 93s 135ms/step - loss: 6.4189\n",
      "Epoch 9/15\n",
      "691/691 [==============================] - 93s 134ms/step - loss: 6.0300\n",
      "Epoch 10/15\n",
      "691/691 [==============================] - 94s 136ms/step - loss: 5.6578\n",
      "Epoch 11/15\n",
      "691/691 [==============================] - 94s 135ms/step - loss: 5.3028\n",
      "Epoch 12/15\n",
      "691/691 [==============================] - 94s 136ms/step - loss: 4.9619\n",
      "Epoch 13/15\n",
      "691/691 [==============================] - 95s 138ms/step - loss: 4.6343\n",
      "Epoch 14/15\n",
      "691/691 [==============================] - 97s 140ms/step - loss: 4.3224\n",
      "Epoch 15/15\n",
      "691/691 [==============================] - 97s 140ms/step - loss: 4.0292\n",
      "Processing 射雕英雄传\n",
      "Epoch 1/15\n",
      "522/522 [==============================] - 72s 137ms/step - loss: 9.4365\n",
      "Epoch 2/15\n",
      "522/522 [==============================] - 72s 139ms/step - loss: 9.0197\n",
      "Epoch 3/15\n",
      "522/522 [==============================] - 72s 139ms/step - loss: 8.8573\n",
      "Epoch 4/15\n",
      "522/522 [==============================] - 73s 139ms/step - loss: 8.5930\n",
      "Epoch 5/15\n",
      "522/522 [==============================] - 72s 138ms/step - loss: 8.2618\n",
      "Epoch 6/15\n",
      "522/522 [==============================] - 73s 139ms/step - loss: 7.9021\n",
      "Epoch 7/15\n",
      "522/522 [==============================] - 73s 140ms/step - loss: 7.5220\n",
      "Epoch 8/15\n",
      "522/522 [==============================] - 72s 138ms/step - loss: 7.1351\n",
      "Epoch 9/15\n",
      "522/522 [==============================] - 71s 137ms/step - loss: 6.7561\n",
      "Epoch 10/15\n",
      "522/522 [==============================] - 72s 138ms/step - loss: 6.3878\n",
      "Epoch 11/15\n",
      "522/522 [==============================] - 72s 139ms/step - loss: 6.0313\n",
      "Epoch 12/15\n",
      "522/522 [==============================] - 74s 141ms/step - loss: 5.6848\n",
      "Epoch 13/15\n",
      "522/522 [==============================] - 70s 133ms/step - loss: 5.3454\n",
      "Epoch 14/15\n",
      "522/522 [==============================] - 71s 136ms/step - loss: 5.0156\n",
      "Epoch 15/15\n",
      "522/522 [==============================] - 72s 137ms/step - loss: 4.6952\n",
      "Processing 神雕侠侣\n",
      "Epoch 1/15\n",
      "641/641 [==============================] - 85s 131ms/step - loss: 9.1493\n",
      "Epoch 2/15\n",
      "641/641 [==============================] - 85s 133ms/step - loss: 8.6994\n",
      "Epoch 3/15\n",
      "641/641 [==============================] - 83s 130ms/step - loss: 8.4303\n",
      "Epoch 4/15\n",
      "641/641 [==============================] - 84s 130ms/step - loss: 8.0622\n",
      "Epoch 5/15\n",
      "641/641 [==============================] - 86s 134ms/step - loss: 7.6564\n",
      "Epoch 6/15\n",
      "641/641 [==============================] - 86s 134ms/step - loss: 7.2319\n",
      "Epoch 7/15\n",
      "641/641 [==============================] - 84s 130ms/step - loss: 6.8037\n",
      "Epoch 8/15\n",
      "641/641 [==============================] - 84s 131ms/step - loss: 6.3938\n",
      "Epoch 9/15\n",
      "641/641 [==============================] - 86s 134ms/step - loss: 6.0084\n",
      "Epoch 10/15\n",
      "641/641 [==============================] - 86s 134ms/step - loss: 5.6414\n",
      "Epoch 11/15\n",
      "641/641 [==============================] - 85s 132ms/step - loss: 5.2880\n",
      "Epoch 12/15\n",
      "641/641 [==============================] - 85s 133ms/step - loss: 4.9465\n",
      "Epoch 13/15\n",
      "641/641 [==============================] - 83s 130ms/step - loss: 4.6170\n",
      "Epoch 14/15\n",
      "641/641 [==============================] - 83s 130ms/step - loss: 4.3006\n",
      "Epoch 15/15\n",
      "641/641 [==============================] - 83s 130ms/step - loss: 3.9981\n",
      "Processing 笑傲江湖\n",
      "Epoch 1/15\n",
      "589/589 [==============================] - 66s 111ms/step - loss: 9.0477\n",
      "Epoch 2/15\n",
      "589/589 [==============================] - 66s 112ms/step - loss: 8.6467\n",
      "Epoch 3/15\n",
      "589/589 [==============================] - 65s 111ms/step - loss: 8.3575\n",
      "Epoch 4/15\n",
      "589/589 [==============================] - 66s 111ms/step - loss: 7.9973\n",
      "Epoch 5/15\n",
      "589/589 [==============================] - 67s 113ms/step - loss: 7.6416\n",
      "Epoch 6/15\n",
      "589/589 [==============================] - 68s 116ms/step - loss: 7.2690\n",
      "Epoch 7/15\n",
      "589/589 [==============================] - 68s 115ms/step - loss: 6.8842\n",
      "Epoch 8/15\n",
      "589/589 [==============================] - 67s 115ms/step - loss: 6.5020\n",
      "Epoch 9/15\n",
      "589/589 [==============================] - 66s 112ms/step - loss: 6.1330\n",
      "Epoch 10/15\n",
      "589/589 [==============================] - 67s 113ms/step - loss: 5.7802\n",
      "Epoch 11/15\n",
      "589/589 [==============================] - 68s 115ms/step - loss: 5.4445\n",
      "Epoch 12/15\n",
      "589/589 [==============================] - 67s 113ms/step - loss: 5.1229\n",
      "Epoch 13/15\n",
      "589/589 [==============================] - 67s 113ms/step - loss: 4.8146\n",
      "Epoch 14/15\n",
      "589/589 [==============================] - 67s 113ms/step - loss: 4.5184\n",
      "Epoch 15/15\n",
      "589/589 [==============================] - 67s 114ms/step - loss: 4.2352\n",
      "Processing 碧血剑\n",
      "Epoch 1/15\n",
      "309/309 [==============================] - 30s 94ms/step - loss: 9.4060\n",
      "Epoch 2/15\n",
      "309/309 [==============================] - 29s 95ms/step - loss: 8.9450\n",
      "Epoch 3/15\n",
      "309/309 [==============================] - 30s 97ms/step - loss: 8.8569\n",
      "Epoch 4/15\n",
      "309/309 [==============================] - 30s 97ms/step - loss: 8.7176\n",
      "Epoch 5/15\n",
      "309/309 [==============================] - 30s 96ms/step - loss: 8.5092\n",
      "Epoch 6/15\n",
      "309/309 [==============================] - 30s 96ms/step - loss: 8.2673\n",
      "Epoch 7/15\n",
      "309/309 [==============================] - 30s 96ms/step - loss: 7.9902\n",
      "Epoch 8/15\n",
      "309/309 [==============================] - 30s 98ms/step - loss: 7.6919\n",
      "Epoch 9/15\n",
      "309/309 [==============================] - 30s 96ms/step - loss: 7.3869\n",
      "Epoch 10/15\n",
      "309/309 [==============================] - 30s 98ms/step - loss: 7.0865\n",
      "Epoch 11/15\n",
      "309/309 [==============================] - 30s 98ms/step - loss: 6.7909\n",
      "Epoch 12/15\n",
      "309/309 [==============================] - 30s 98ms/step - loss: 6.5010\n",
      "Epoch 13/15\n",
      "309/309 [==============================] - 30s 97ms/step - loss: 6.2159\n",
      "Epoch 14/15\n",
      "309/309 [==============================] - 30s 98ms/step - loss: 5.9372\n",
      "Epoch 15/15\n",
      "309/309 [==============================] - 31s 99ms/step - loss: 5.6618\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/'\n",
    "text_names = ['倚天屠龙记', '鹿鼎记', '射雕英雄传', '神雕侠侣', '笑傲江湖','碧血剑']\n",
    "\n",
    "# 获取停用词列表\n",
    "stop_words_list = text_stop(file_path)\n",
    "\n",
    "# 保存处理后的文本\n",
    "output_dir = \"output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    # Train models and extract embeddings\n",
    "models_data = []\n",
    "for name in text_names:\n",
    "    text_data = process_text_file(file_path, name, stop_words_list)\n",
    "    tokenizer, embeddings, word_index = train_model(name, text_data)\n",
    "    models_data.append((name, tokenizer, embeddings, word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e018960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words similar to '张无忌' in '倚天屠龙记':\n",
      "周芷若 0.8084123\n",
      "殷素素 0.805586\n",
      "张翠山 0.78017986\n",
      "赵敏 0.7621716\n",
      "都大锦 0.74168056\n",
      "俞岱岩 0.7367599\n",
      "宋青书 0.72280306\n",
      "蛛儿 0.71801317\n",
      "卫璧 0.7154796\n",
      "金花婆婆 0.70907915\n",
      "\n",
      "Top 10 words similar to '韦小宝' in '鹿鼎记':\n",
      "康熙 0.8837671\n",
      "吴之荣 0.8103085\n",
      "陈近南 0.80448043\n",
      "老者 0.798382\n",
      "女尼 0.78054684\n",
      "茅十八 0.77979743\n",
      "张康年 0.7717519\n",
      "图尔布青 0.76906645\n",
      "海老公 0.7667867\n",
      "陶红英 0.76284593\n",
      "\n",
      "Top 10 words similar to '郭靖' in '射雕英雄传':\n",
      "黄蓉 0.7820635\n",
      "众人 0.7283165\n",
      "六子 0.714378\n",
      "欧阳克 0.6903978\n",
      "黄药师 0.6585986\n",
      "洪七公 0.65272653\n",
      "穆易 0.64841056\n",
      "韩宝驹 0.6455806\n",
      "丘处机 0.6448091\n",
      "陆冠英 0.6433101\n",
      "\n",
      "Top 10 words similar to '杨过' in '神雕侠侣':\n",
      "小龙女 0.8335143\n",
      "李莫愁 0.8033289\n",
      "陆无双 0.798977\n",
      "法王 0.7841845\n",
      "周伯通 0.77406096\n",
      "赵志敬 0.76541376\n",
      "柯镇恶 0.7649498\n",
      "裘千尺 0.75431204\n",
      "完颜萍 0.7445578\n",
      "郭襄 0.743217\n",
      "\n",
      "Top 10 words similar to '令狐冲' in '笑傲江湖':\n",
      "岳不群 0.80036384\n",
      "桃花仙 0.79262125\n",
      "盈盈 0.7918447\n",
      "任行 0.78677994\n",
      "向问天 0.7818441\n",
      "岳夫人 0.75931877\n",
      "玉玑子 0.7569629\n",
      "林平之 0.75383526\n",
      "定静师太 0.75357324\n",
      "岳灵珊 0.7523141\n",
      "\n",
      "Top 10 words similar to '袁承志' in '碧血剑':\n",
      "崔秋山 0.8582802\n",
      "青青 0.7778281\n",
      "焦公礼 0.7749132\n",
      "温青 0.77036595\n",
      "张朝唐 0.7654547\n",
      "温仪 0.76339984\n",
      "何红药 0.751696\n",
      "单铁生 0.7482195\n",
      "洪胜海 0.74631274\n",
      "何铁手 0.74528486\n"
     ]
    }
   ],
   "source": [
    "test_name_mapping = {\n",
    "        '倚天屠龙记': '张无忌',\n",
    "        '鹿鼎记':'韦小宝',\n",
    "        '天龙八部': '乔峰',\n",
    "        '射雕英雄传': '郭靖',\n",
    "        '神雕侠侣': '杨过',\n",
    "        '笑傲江湖': '令狐冲',\n",
    "        '碧血剑':'袁承志'\n",
    "    }\n",
    "\n",
    "for name, tokenizer, embeddings, word_index in models_data:\n",
    "    test_word = test_name_mapping[name]\n",
    "    if test_word in word_index:\n",
    "        print(f\"\\nTop 10 words similar to '{test_word}' in '{name}':\")\n",
    "        test_word_index = word_index[test_word]\n",
    "        test_word_vector = embeddings[test_word_index].reshape(1, -1)\n",
    "        similarities = cosine_similarity(test_word_vector, embeddings)[0]\n",
    "        similar_indices = similarities.argsort()[-11:-1][::-1]\n",
    "        similar_words = [(tokenizer.index_word[idx], similarities[idx]) for idx in similar_indices]\n",
    "        for word, similarity in similar_words:\n",
    "            print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f76ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
