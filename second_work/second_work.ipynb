{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9c58131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e235db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset(corpus, labels, num_paragraphs):\n",
    "    # 计算每个标签需要抽取的段落数量\n",
    "    num_paragraphs_per_label = num_paragraphs // len(set(labels))\n",
    "    dataset = []\n",
    "    dataset_labels = []\n",
    "    for label in set(labels):\n",
    "        # 从具有特定标签的段落中均匀抽取指定数量的段落\n",
    "        label_paragraphs = [paragraph for paragraph, paragraph_label in zip(corpus, labels) if paragraph_label == label]\n",
    "        sampled_paragraphs = np.random.choice(label_paragraphs, num_paragraphs_per_label, replace=False)\n",
    "        dataset.extend(sampled_paragraphs)\n",
    "        dataset_labels.extend([label] * num_paragraphs_per_label)\n",
    "    return dataset, dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec17c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取语料库，假设语料库是一个包含小说段落的文件，每行为一个段落\n",
    "folder_path = \"data\"\n",
    "\n",
    "    # 读取文件夹下所有txt文件的内容并合并成一个语料库\n",
    "corpus = []\n",
    "labels = []\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder_path, file_name), \"r\",encoding='gb18030') as file:\n",
    "            text = file.read()\n",
    "            paragraphs = text.split(\"\\n\")\n",
    "            corpus.extend(paragraphs)\n",
    "            labels.extend([\"book_\" + file_name.split(\".\")[0]] * len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "102add1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 定义不同的 K\n",
    "K_values = [20, 100, 500, 1000, 3000]\n",
    "\n",
    "    # 定义不同的主题数量 T\n",
    "T_values = [5, 10,25, 50, 100]\n",
    "\n",
    "    # 定义交叉验证的次数\n",
    "num_cross_val = 10\n",
    "\n",
    "    # 定义分类器\n",
    "classifiers = {\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"SVM\": SVC(),\n",
    "        \"XGB\": XGBClassifier()\n",
    "    }\n",
    "\n",
    "    # 定义结果存储列表\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8938f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "dataset, dataset_labels = extract_dataset(corpus, labels, num_paragraphs=1000)\n",
    "\n",
    "# 将字符串类别标签编码为整数类别标签\n",
    "label_encoder = LabelEncoder()\n",
    "dataset_labels = label_encoder.fit_transform(dataset_labels)\n",
    "# 将数据集划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, dataset_labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d02173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:03<00:15,  3.98s/it]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:07<00:10,  3.48s/it]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [00:10<00:07,  3.57s/it]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [00:14<00:03,  3.72s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:20<00:00,  4.06s/it]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:20<01:21, 20.28s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:04<00:16,  4.22s/it]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:08<00:12,  4.31s/it]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [00:13<00:08,  4.49s/it]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [00:18<00:04,  4.66s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:24<00:00,  4.86s/it]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:44<01:07, 22.66s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:04<00:17,  4.35s/it]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:09<00:15,  5.07s/it]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [00:14<00:09,  4.83s/it]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [00:20<00:05,  5.45s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:28<00:00,  5.62s/it]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [01:12<00:50, 25.15s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:04<00:18,  4.54s/it]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:10<00:16,  5.36s/it]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [00:15<00:10,  5.12s/it]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [00:21<00:05,  5.61s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:29<00:00,  5.99s/it]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [01:42<00:27, 27.04s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:05<00:20,  5.01s/it]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:11<00:16,  5.59s/it]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [00:16<00:10,  5.49s/it]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [00:23<00:06,  6.08s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:32<00:00,  6.51s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [02:15<00:00, 27.04s/it]\n"
     ]
    }
   ],
   "source": [
    "for K in tqdm(K_values):\n",
    "    for T in tqdm(T_values):\n",
    "        lda_pipeline = Pipeline([\n",
    "                ('vectorizer', CountVectorizer(max_features=K, analyzer='word')),\n",
    "                ('lda', LatentDirichletAllocation(n_components=T, random_state=42))\n",
    "            ])\n",
    "\n",
    "            # 将文本转换为主题分布\n",
    "        X_train_lda = lda_pipeline.fit_transform(X_train)\n",
    "        X_test_lda = lda_pipeline.transform(X_test)\n",
    "\n",
    "            # 使用不同的分类器进行训练和评估\n",
    "        for classifier_name, classifier in classifiers.items():\n",
    "                # 保存结果\n",
    "\n",
    "            classifier.fit(X_train_lda, y_train)\n",
    "            accuracy = np.mean(cross_val_score(classifier, X_train_lda, y_train, cv=num_cross_val))\n",
    "            test_accuracy = accuracy_score(y_test, classifier.predict(X_test_lda))\n",
    "\n",
    "                # 保存结果\n",
    "\n",
    "            results.append({\n",
    "                    'K': K,\n",
    "                    'T': T,\n",
    "                    'Classifier': classifier_name,\n",
    "                    'Analyzer': 'Word',\n",
    "                    'Training Accuracy': accuracy,\n",
    "                    'Test Accuracy': test_accuracy\n",
    "                })\n",
    "\n",
    "    # 将结果转换为DataFrame\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "    # 保存结果到xlsx文件\n",
    "results.to_excel(\"result.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf0033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
